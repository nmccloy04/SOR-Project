---
title: "Bayesian Statistics Project "
AuthoR: "Niamh McCloy"
output: html_notebook
---

# Introduction
This notebook demonstrates Bayesian linear regression using importance sampling, Metropolis-Hastings, and Gibbs sampling. We simulate data and estimate posterior distributions for model parameters.


This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

Importance sampling for Bayesian Linear regression (example in notes)

# Libraries and Setup
```{r}
# The easiest way to get ggplot2 is to install the whole tidyverse:
#install.packages("tidyverse")
# Alternatively, install just ggplot2:


# Load necessary libraries and install if not already available
# Uncomment install.packages() lines if required

# install.packages("tidyverse")
# install.packages("ggplot2")
install.packages("tictoc")


```
```{r}
rm(list = ls()) #clear workspace
library(ggplot2)
library('tictoc')
```



The model that we are trying to fit is

y=alpha+beta*x+beta2*x2+epsilon

epsilon~N(0, 1/tau)

tau=1/variance

We want to take 2000 posterior samples  based on 100000 simulated sets of parameter values from the prior

```{r}
K<-100000 #No. of prior samples
nsamples <-2000 # No. of posterior samples to take

pred_point <-3.0 # value at which to compute the predictive distribution
```


Set up of true parameter values, generate data from these (you won't know these if you where given a data set but in a simulation study you want to show your code can give these values)

```{r}
truealpha= 2.5
truebeta= 3.0
truebeta2= 0.8
truetau= 1
```

Prior parameters: normal on alpha and beta, gamma on tau
```{r}
alphamean= 0
alphavar= 100
betamean= 0
betavar= 100
beta2mean= 0
beta2var= 100
taushape= 0.5
tauscale= 0.5
```

To set up the data we need to use the true parameter values and set up what x is going to be

```{r}
x = rnorm(200, 0, sd=1.25)
x2 = rnorm(200, 2, sd=1.25)
n = length(x)
y<-truealpha+truebeta*x+truebeta2*x2+rnorm(n,0,(sqrt(1/truetau)))
data<-data.frame(y,x,x2)

ggplot(data, aes(x,y))+geom_point()
ggplot(data, aes(x2,y))+geom_point()
```


Using importance sampling

```{r}
tic('IS') #start timing
set.seed(21) #reproducibility
Results<-list()
logweights<-list()
alpha<-numeric()
beta<-numeric()
beta2<-numeric()
tau<-numeric()
sumalpha<-numeric()
sumbeta<-numeric()
sumbeta2<- numeric()
sumtau<-numeric()
sumalpha1<-numeric()
sumbeta1<-numeric()
sumbeta21<-numeric()
sumtau1<-numeric()
sum_of_residuals<-numeric()


for (i in 1:nsamples){
  
  ##prior generation of K samples
    alphasamples <- rnorm(K,alphamean,sqrt(alphavar))
    betasamples <- rnorm(K,betamean,sqrt(betavar))
    beta2samples <- rnorm(K, beta2mean, sqrt(beta2var))
    tausamples <- rgamma(K,taushape,tauscale)
 
    ###computing the logweights 
    z <- (1/2)*n*log(tausamples)-(1/2)*n*log(2*pi)
    for (j in 1:n){
        likelihood <- y[j] - alphasamples - betasamples*x[j] - beta2samples*x2[j]
        logweights[[j]] <- (-(tausamples/2))*(likelihood^2)
    }
    #Calculating weights
  logweights <- data.frame(logweights)
  logweights1 <- rowSums(logweights)
  logweights2 <- z + logweights1
  
  #max_logweight<- max(logweights2)
  #logweights2 <- logweights2 - max_logweight
  
  likelihoodlums <- exp(logweights2)
  weights <- (likelihoodlums/sum(likelihoodlums))
  
  sumalpha[i] <- sum(alphasamples*weights)
  sumbeta[i] <- sum(betasamples*weights)
  sumbeta2[i] <- sum(beta2samples*weights) 
  sumtau[i] <- sum(tausamples*weights)
}
toc()

```

Posterior summaries

```{r}
#compute posterior means and credible intervals
posterior_means = cbind(mean(sumalpha), mean(sumbeta), mean(sumbeta2), mean(sumtau))
posterior_means
quantile(sumalpha,0.025)
quantile(sumalpha,0.975)

quantile(sumbeta,0.025)
quantile(sumbeta,0.975)

quantile(sumbeta2,0.025)
quantile(sumbeta2,0.975)

quantile(sumtau,0.025)
quantile(sumtau,0.975)

posterior1<- cbind((sumalpha), (sumbeta), (sumbeta2), (sumtau))


```

```{r}
posterior_means = cbind(mean(sumalpha), mean(sumbeta),mean(sumbeta2), mean(sumtau))
posterior_means
posterior_lb = cbind(quantile(sumalpha,0.025), quantile(sumbeta,0.025), quantile(sumbeta2,0.025), quantile(sumtau,0.025))
posterior_lb
posterior_ub = cbind(quantile(sumalpha,0.975), quantile(sumbeta,0.975), quantile(sumbeta2,0.975), quantile(sumtau,0.975))
posterior_ub
```


Estimating the normalising constant
```{r}
alphasamples <- rnorm(K,alphamean,sqrt(alphavar))
betasamples <- rnorm(K,betamean,sqrt(betavar))
beta2samples <- rnorm(K, beta2mean, sqrt(beta2var))
tausamples <- rgamma(K,taushape,tauscale)
norm_temp <- 0.5*n*log(tausamples/(2*pi));


for (j in 1:n){
  
    norm_temp = norm_temp - tausamples*0.5*((y[j]-alphasamples - betasamples*x[j] - beta2samples*x2[j])^2)
    # Print the values of norm_temp for debugging (optional)
  if (j == 1) {
    print(head(norm_temp))
  }
}

# Check if norm_temp contains extreme values (for debugging)
summary(norm_temp)  # Get summary of norm_temp values before exponentiating

norm_constant = mean(exp(norm_temp));
norm_constant
summary(tausamples)

residuals = y - alphasamples - betasamples * x - beta2samples * x2
summary(residuals)  # Check for large residuals
hist(residuals)     # Visualise residuals
```
```{r}
##plotting the frequency of alpha, beta and tau

#hist(posterior1[,1], xlab="alpha")
#hist(posterior1[,2], xlab="beta")
#hist(posterior1[,3], xlab = "beta2")
#hist(posterior1[,4], xlab="tau")


par(mfrow = c(2,2))
hist(posterior1[,1],nclass=20, main="Posterior of Alpha", xlab="True value = red line" )
abline(v = mean(posterior1[,1]), col="blue")
abline(v = truealpha, col="red" )
hist(posterior1[,2],nclass=20, main="Posterior of Beta1", xlab="True value = red line" )
abline(v = mean(posterior1[,2]), col="blue")
abline(v = truebeta, col="red" )
hist(posterior1[,3],nclass=20, main="Posterior of Beta2", xlab="True value = red line" )
abline(v = mean(posterior1[,3]), col="blue")
abline(v = truebeta2, col="red" )
hist(posterior1[,4],nclass=20, main="Posterior of Tau", xlab="True value = red line" )
abline(v = mean(posterior1[,4]), col="blue")
abline(v = truetau, col="red" )




```




LSmeans comparison
```{r}
#install.packages("lsmeans")
library(lsmeans)
```
```{r}
set.seed(21)
lsmeans_results2<-lm(y~x+x2, data)
summary(lsmeans_results2)
confint(lsmeans_results2)

anova(lsmeans_results2)
sum(lsmeans_results2$residuals^2)/qchisq(0.975,df=21)
sum(lsmeans_results2$residuals^2)/qchisq(0.025,df=21)
```
```{r}
prediction_y<-lsmeans_results$coefficients[1]+lsmeans_results$coefficients[2]*pred_point+rnorm(1,0,0.8496^2)
```


#########################################################
MCMC approaches``

1- Metropolis Hasting

Using the same data try and fit the model using the Metropolis Hastings algorithm. Assume normal priors on alpha and beta, and a uniform prior on the sd.

sd=sqrt(1/tau)



```{r}
###Likelihood set up as a function


likelihood = function(param){
    a = param[1]
    b = param[2]
    c = param[3]
    sd = param[4]
    
 
    pred = a + b*x + c*x2
    singlelikelihoods = dnorm(y, mean = pred, sd = (sd), log = T)
    sumll = sum(singlelikelihoods)
    return(sumll)
}
```







```{r}
###prior set up
prior = function(param){
    a = param[1]
    b = param[2]
    c = param[3]
    sd = param[4]
    
    aprior = dunif(a, min=0, max=10, log =T)
    bprior = dnorm(b, sd = 5, log = T)
    cprior = dnorm(c, sd=5, log= T)
    sdprior = dunif(sd, min=0, max=10, log = T)
    return(aprior+bprior+cprior+sdprior)
}
```




```{r}
####posterior set up
posterior = function(param){
  return (likelihood(param) +prior(param))
}
```


```{r}
###proposal set up
proposalfunction = function(param){
   mhalpha = param[1]
   mhbeta = param[2]
   mhc = param[3]
   mhtau = param[4]
   
 #  
 #  
   aproposal<-rnorm(1,mhalpha,0.25^2)
   bproposal<-rnorm(1, mhbeta, 0.25^2)
   cproposal<-rnorm(1, mhc, 0.25^2)
   Tproposal<-runif(1, max(0, mhtau-.2), mhtau+.2)
  
 # 
   proposals<-c(aproposal,bproposal, cproposal, Tproposal)
  return(proposals)
}
```




```{r}
####function call

run_metropolis_MCMC = function(startvalue, iterations){
    chain = array(dim = c(iterations+1,4))
    chain[1,] = startvalue
    for (i in 1:iterations){
        proposal = proposalfunction(chain[i,])
 
        probab = exp(posterior(proposal) - posterior(chain[i,]))
       
        if (runif(1) < probab){
            chain[i+1,] = proposal
        }else{
            chain[i+1,] = chain[i,]
        }
    }
    return(chain)
}
 
```
```{r}
###running a call
tic('MH') #start timing
startvalue = c(4,0,10,2) #initial values
chain = run_metropolis_MCMC(startvalue, 1000000)
 
burnIn = 50000
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))

MHalpha<-mean(chain[burnIn:100000,1])
MHbeta<-mean(chain[burnIn:100000,2])
MHc<-mean(chain[burnIn:100000,3])
MHTau<-mean(chain[burnIn:100000,4])
toc()


MHposterior_means = cbind(mean(chain[-(1:burnIn),1]), mean(chain[-(1:burnIn),2]), mean(chain[-(1:burnIn),3]), mean(chain[-(1:burnIn),4]))
MHposterior_means
quantile(chain[-(1:burnIn),1],0.025)
quantile(chain[-(1:burnIn),1],0.975)

quantile(chain[-(1:burnIn),2],0.025)
quantile(chain[-(1:burnIn),2],0.975)

quantile(chain[-(1:burnIn),3],0.025)
quantile(chain[-(1:burnIn),3],0.975)

quantile(chain[-(1:burnIn),4],0.025)
quantile(chain[-(1:burnIn),4],0.975)



```

```{r}
par(mfrow = c(2,4))
hist(chain[-(1:burnIn),1],nclass=30, , main="Posterior of Alpha", xlab="True value = red line" )
abline(v = mean(chain[-(1:burnIn),1]), col="blue")
abline(v = truealpha, col="red" )
hist(chain[-(1:burnIn),2],nclass=30, main="Posterior of Beta1", xlab="True value = red line")
abline(v = mean(chain[-(1:burnIn),2]), col="blue")
abline(v = truebeta, col="red" )
hist(chain[-(1:burnIn),3],nclass=30, main="Posterior of Beta2", xlab="True value = red line")
abline(v = mean(chain[-(1:burnIn),3]), col="blue")
abline(v = truebeta2, col="red" )
hist(chain[-(1:burnIn),4],nclass=30, main="Posterior of Tau", xlab="True value = red line")
abline(v = mean(chain[-(1:burnIn),4]), col="blue" )
abline(v = truetau, col="red" )
plot(chain[-(1:burnIn),1], type = "l", xlab="True value = red line" , main = "Chain values of Alpha", )
abline(h = truealpha, col="red" )
plot(chain[-(1:burnIn),2], type = "l", xlab="True value = red line" , main = "Chain values of Beta1", )
abline(h = truebeta, col="red" )
plot(chain[-(1:burnIn),3], type = "l", xlab="True value = red line" , main = "Chain values of Beta2", )
abline(h = truebeta2, col="red" )
plot(chain[-(1:burnIn),4], type = "l", xlab="True value = red line" , main = "Chain values of Tau", )
abline(h = truetau, col="red" )
```



Gibbs Sampler:
```{r}
# Initialise parameters and variables
n_iter <- 11000  # Number of iterations
burn_in <- 1000  # Number of burn-in iterations
```

```{r}
# Initial values for Gibbs sampling
alpha <- 2
beta <- 1
beta2 <- 0.2
tau <- 0.5
```



```{r}
# Initialise parameters and variables
tic('GS') #Start timing
set.seed(21)  #Reproducibility


alpha_samples <- beta_samples <- beta2_samples <- tau_samples <- numeric(n_iter)

# Gibbs sampling
for (i in 1:n_iter) {
  # Sample alpha
  alpha_var <- 1 / ((1 / alphavar) + n * tau)
  alpha_mean <- alpha_var * (alphamean/alphavar + tau * sum(y - beta*x - beta2*x2))
  alpha <- rnorm(1, mean = alpha_mean, sd = sqrt(alpha_var))
  
  # Sample beta1
  beta_var <- 1 / (1/betavar + tau * sum(x^2))
  beta_mean <- beta_var * (betamean/betavar+ tau * sum(x * (y - alpha - beta2 * x2)))
  beta <- rnorm(1, mean = beta_mean, sd = sqrt(beta_var))
  
  # Sample beta2
  beta2_var <- 1 / (1/beta2var + tau * sum(x2^2))
  beta2_mean <- beta2_var * (beta2mean/beta2var+tau * sum(x2 * (y - alpha - beta * x)))
  beta2 <- rnorm(1, mean = beta2_mean, sd = sqrt(beta2_var))
  
  # Sample tau
  tau_shape <- 2 + n / 2
  tau_rate <-0.5+ 0.5 * sum((y - alpha - beta * x - beta2 * x2)^2)
  tau <- rgamma(1, shape = tau_shape, rate = tau_rate)
  
  # Store samples
  alpha_samples[i] <- alpha
  beta_samples[i] <- beta
  beta2_samples[i] <- beta2
  tau_samples[i] <- tau
}

# Discard burn-in samples
alpha_samples <- alpha_samples[-(1:burn_in)]
beta_samples <- beta_samples[-(1:burn_in)]
beta2_samples <- beta2_samples[-(1:burn_in)]
tau_samples <- tau_samples[-(1:burn_in)]

# Calculate credible intervals for each parameter
credible_intervals_alpha <- quantile(alpha_samples, c(0.025, 0.975))
credible_intervals_beta <- quantile(beta_samples, c(0.025, 0.975))
credible_intervals_beta2 <- quantile(beta2_samples, c(0.025, 0.975))
credible_intervals_tau <- quantile(tau_samples, c(0.025, 0.975))

# Print credible intervals
print("Credible Intervals:")
print(paste("Alpha:", credible_intervals_alpha[1], "-", credible_intervals_alpha[2]))
print(paste("Beta1:", credible_intervals_beta[1], "-", credible_intervals_beta[2]))
print(paste("Beta2:", credible_intervals_beta2[1], "-", credible_intervals_beta2[2]))
print(paste("Tau:", credible_intervals_tau[1], "-", credible_intervals_tau[2]))

toc()

# Plot histograms
par(mfrow = c(2, 2))
hist(alpha_samples, main = "Alpha")
hist(beta_samples, main = "Beta1")
hist(beta2_samples, main = "Beta2")
hist(tau_samples, main = "Tau")
par(mfrow = c(1, 1))


summary(alpha_samples)
summary(beta_samples)
summary(beta2_samples)
summary(tau_samples)


gsposterior_means = cbind(mean(alpha_samples), mean(beta_samples), mean(beta2_samples), mean(tau_samples))
gsposterior_means
quantile(alpha_samples,0.025)
quantile(alpha_samples,0.975)

quantile(beta_samples,0.025)
quantile(beta_samples,0.975)

quantile(beta2_samples,0.025)
quantile(beta2_samples,0.975)

quantile(tau_samples,0.025)
quantile(tau_samples,0.975)

gsposterior1<- cbind((alpha_samples), (beta_samples), (beta2_samples), (tau_samples))


par(mfrow = c(2,2))
hist(gsposterior1[,1],nclass=20, main="Posterior of Alpha", xlab="True value = red line" )
abline(v = mean(gsposterior1[,1]), col="blue")
abline(v = truealpha, col="red" )
hist(gsposterior1[,2],nclass=20, main="Posterior of Beta1", xlab="True value = red line" )
abline(v = mean(gsposterior1[,2]), col="blue")
abline(v = truebeta, col="red" )
hist(gsposterior1[,3],nclass=20, main="Posterior of Beta2", xlab="True value = red line" )
abline(v = mean(gsposterior1[,3]), col="blue")
abline(v = truebeta2, col="red" )
hist(gsposterior1[,4],nclass=20, main="Posterior of Tau", xlab="True value = red line" )
abline(v = mean(gsposterior1[,4]), col="blue")
abline(v = truetau, col="red" )

```


Metropolis-Hastings built-in
```{r}
#Installing packages
install.packages("metropolis")
library(metropolis)
library(coda)
```


```{r}
#Chunk 23
# Create a formula for the model
formula <- y ~ x + x2

# Run Metropolis-Hastings sampling using metropolis.glm
tic('MHBI')
set.seed(21)  #Reproducibility
res = metropolis_glm(formula, data=data, family=gaussian(), iter=200000, burnin=100000,
adapt=TRUE, guided=TRUE, block=FALSE)
res2 = as.mcmc(res)
summary(res2)
traceplot(res2)

# Extract posterior samples for parameters (b_0, b_1, b_2, logsigma)
b0_samples <- res2[, "b_0"]   # Extract b_0 (alpha)
b1_samples <- res2[, "b_1"]   # Extract b_1 (beta1)
b2_samples <- res2[, "b_2"]   # Extract b_2 (beta2)
logsigma_samples <- res2[, "logsigma"]  # Extract logsigma

# Convert logsigma to tau (since tau = 1 / sigma^2, and sigma = exp(logsigma))
sigma_samples <- exp(logsigma_samples)
tau_samples <- 1 / sigma_samples

# Combine all posterior samples into a matrix for plotting
mhposterior <- cbind(b0_samples, b1_samples, b2_samples, tau_samples)
toc()
# Set up plotting area
par(mfrow = c(2, 2))

# Plot posterior for b_0 (alpha)
hist(mhposterior[, 1], nclass = 20, main = "Posterior of Alpha", xlab = "True value = red line")
abline(v = mean(mhposterior[, 1]), col = "blue")  # Posterior mean in blue
abline(v = truealpha, col = "red")  # True value in red

# Plot posterior for b_1 (beta1)
hist(mhposterior[, 2], nclass = 20, main = "Posterior of Beta1", xlab = "True value = red line")
abline(v = mean(mhposterior[, 2]), col = "blue")
abline(v = truebeta, col = "red")

# Plot posterior for b_2 (beta2)
hist(mhposterior[, 3], nclass = 20, main = "Posterior of Beta2", xlab = "True value = red line")
abline(v = mean(mhposterior[, 3]), col = "blue")
abline(v = truebeta2, col = "red")

# Plot posterior for tau
hist(mhposterior[, 4], nclass = 20, main = "Posterior of Tau", xlab = "True value = red line")
abline(v = mean(mhposterior[, 4]), col = "blue")
abline(v = truetau, col = "red")

traceplot(mhposterior[, 1],type = "l", xlab="True value = red line" , main = "Chain values of Alpha", )  # Extract b_0 (alpha)
abline(h = truealpha, col="red" )
traceplot(mhposterior[, 2],type = "l", xlab="True value = red line" , main = "Chain values of Beta", )   # Extract b_1 (beta1)
abline(h = truebeta, col="red" )
traceplot(mhposterior[, 3],type = "l", xlab="True value = red line" , main = "Chain values of Beta2", )   # Extract b_2 (beta2)
abline(h = truebeta2, col="red" )
traceplot(mhposterior[, 4],type = "l", xlab="True value = red line" , main = "Chain values of Tau", )
abline(h = truetau, col="red" )

```

Gibs Sampler Built-in
```{r}
install.packages("MCMCpack")
tic('GSBI')
set.seed(21)
library(MCMCpack)

data_list <- list(
  y = y,
  x1 = x,
  x2 = x2
)


gibbs_output <- MCMCregress(y ~ x1 + x2, data = data_list, b0 = rep(0, 3), B0 = diag(1/10, 3), m0 = rep(0, 3), verbose = TRUE)


summary(gibbs_output)

# Extract coefficients from the gibbs_output object
coefficients <- as.matrix(gibbs_output)
toc()
# Plot histograms for the intercept, coefficients, and sigma2
par(mfrow=c(2, 2))  


hist(coefficients[, 1], main="Estimate for alpha", xlab = "True value = red line")
abline(v = mean(coefficients[, 1]), col = "blue")
abline(v = truealpha, col = "red")


hist(coefficients[, 2], main="Estimate for beta1", xlab = "True value = red line")
abline(v = mean(coefficients[, 2]), col = "blue")
abline(v = truebeta, col = "red")

hist(coefficients[, 3], main="Estimate for beta2", xlab = "True value = red line")
abline(v = mean(coefficients[, 3]), col = "blue")
abline(v = truebeta2, col = "red")
sigma2_values <- gibbs_output[, "sigma2"]


hist(sigma2_values, main="Estimate for tau", xlab = "True value = red line")
abline(v = mean(sigma2_values), col = "blue")
abline(v = truetau, col = "red")
```
Sensitivity Analysis



Importance Sampling
```{r}
#Modify Iterations
# Fewer iterations
K<-10000 #No. of prior samples
nsamples <-200 # No. of posterior samples to take

```

```{r}
#Modify Iterations
#More iterations
K<-200000 #No. of prior samples
nsamples <-3000 # No. of posterior samples to take

```




```{r}
#Investigate Priors
#Using the original number of iterations

alphamean= 0
alphavar= 100
betamean= 0
betavar= 100
beta2mean= 0
beta2var= 100
taushape= 1
tauscale= 1

```

```{r}
#Investigate Priors
#Using the original number of iterations

alphamean= 0
alphavar= 100
betamean= 0
betavar= 100
beta2mean= 0
beta2var= 100
taushape= 2
tauscale= 2
```

```{r}
tic('IS') #start timing
set.seed(21) #reproducibility
Results<-list()
logweights<-list()
alpha<-numeric()
beta<-numeric()
beta2<-numeric()
tau<-numeric()
sumalpha<-numeric()
sumbeta<-numeric()
sumbeta2<- numeric()
sumtau<-numeric()
sumalpha1<-numeric()
sumbeta1<-numeric()
sumbeta21<-numeric()
sumtau1<-numeric()
sum_of_residuals<-numeric()


for (i in 1:nsamples){
  
  ##prior generation of K samples
    alphasamples <- rnorm(K,alphamean,sqrt(alphavar))
    betasamples <- rnorm(K,betamean,sqrt(betavar))
    beta2samples <- rnorm(K, beta2mean, sqrt(beta2var))
    tausamples <- runif(K,min =1,max=10)
 
    ###computing the logweights 
    z <- (1/2)*n*log(tausamples)-(1/2)*n*log(2*pi)
    for (j in 1:n){
        likelihood <- y[j] - alphasamples - betasamples*x[j] - beta2samples*x2[j]
        logweights[[j]] <- (-(tausamples/2))*(likelihood^2)
    }
    #Calculating weights
  logweights <- data.frame(logweights)
  logweights1 <- rowSums(logweights)
  logweights2 <- z + logweights1
  
  #max_logweight<- max(logweights2)
  #logweights2 <- logweights2 - max_logweight
  
  likelihoodlums <- exp(logweights2)
  weights <- (likelihoodlums/sum(likelihoodlums))
  
  sumalpha[i] <- sum(alphasamples*weights)
  sumbeta[i] <- sum(betasamples*weights)
  sumbeta2[i] <- sum(beta2samples*weights) 
  sumtau[i] <- sum(tausamples*weights)
}
toc()

```



Metropolis Hastings

```{r}
#Modify Iterations and burn-in
###running a call
tic('MH') #start timing
startvalue = c(4,0,10,2) #initial values
chain = run_metropolis_MCMC(startvalue, 1500000)#1.5 milll instead of 1mill
 
burnIn = 50000 #50,000
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))

MHalpha<-mean(chain[burnIn:1500000,1])
MHbeta<-mean(chain[burnIn:1500000,2])
MHc<-mean(chain[burnIn:1500000,3])
MHTau<-mean(chain[burnIn:1500000,4])
toc()


MHposterior_means = cbind(mean(chain[-(1:burnIn),1]), mean(chain[-(1:burnIn),2]), mean(chain[-(1:burnIn),3]), mean(chain[-(1:burnIn),4]))
MHposterior_means
quantile(chain[-(1:burnIn),1],0.025)
quantile(chain[-(1:burnIn),1],0.975)

quantile(chain[-(1:burnIn),2],0.025)
quantile(chain[-(1:burnIn),2],0.975)

quantile(chain[-(1:burnIn),3],0.025)
quantile(chain[-(1:burnIn),3],0.975)

quantile(chain[-(1:burnIn),4],0.025)
quantile(chain[-(1:burnIn),4],0.975)




```





```{r}
#Modify Iterations and burn-in
###running a call
tic('MH') #start timing
startvalue = c(4,0,10,2) #initial values
chain = run_metropolis_MCMC(startvalue, 100000)#100,000 instead of 1mill
 
burnIn = 5000 #5000, instead of 50,000
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))

MHalpha<-mean(chain[burnIn:10000,1])
MHbeta<-mean(chain[burnIn:10000,2])
MHc<-mean(chain[burnIn:10000,3])
MHTau<-mean(chain[burnIn:10000,4])
toc()


MHposterior_means = cbind(mean(chain[-(1:burnIn),1]), mean(chain[-(1:burnIn),2]), mean(chain[-(1:burnIn),3]), mean(chain[-(1:burnIn),4]))
MHposterior_means
quantile(chain[-(1:burnIn),1],0.025)
quantile(chain[-(1:burnIn),1],0.975)

quantile(chain[-(1:burnIn),2],0.025)
quantile(chain[-(1:burnIn),2],0.975)

quantile(chain[-(1:burnIn),3],0.025)
quantile(chain[-(1:burnIn),3],0.975)

quantile(chain[-(1:burnIn),4],0.025)
quantile(chain[-(1:burnIn),4],0.975)




```
```{r}
#Modify Iterations and burn-in
###running a call
tic('MH') #start timing
startvalue = c(4,0,10,2) #initial values
chain = run_metropolis_MCMC(startvalue, 10000)#100,000 instead of 1mill
 
burnIn = 500 #5000, instead of 50,000
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))

MHalpha<-mean(chain[burnIn:10000,1])
MHbeta<-mean(chain[burnIn:10000,2])
MHc<-mean(chain[burnIn:10000,3])
MHTau<-mean(chain[burnIn:10000,4])
toc()


MHposterior_means = cbind(mean(chain[-(1:burnIn),1]), mean(chain[-(1:burnIn),2]), mean(chain[-(1:burnIn),3]), mean(chain[-(1:burnIn),4]))
MHposterior_means
quantile(chain[-(1:burnIn),1],0.025)
quantile(chain[-(1:burnIn),1],0.975)

quantile(chain[-(1:burnIn),2],0.025)
quantile(chain[-(1:burnIn),2],0.975)

quantile(chain[-(1:burnIn),3],0.025)
quantile(chain[-(1:burnIn),3],0.975)

quantile(chain[-(1:burnIn),4],0.025)
quantile(chain[-(1:burnIn),4],0.975)



```




```{r}
#Investigate Priors
#trying a gamma on tau
###prior set up
prior = function(param){
    a = param[1]
    b = param[2]
    c = param[3]
    sd = param[4]
    
    aprior = dunif(a, min=0, max=10, log =T)
    bprior = dnorm(b, sd = 5, log = T)
    cprior = dnorm(c, sd=5, log= T)
    sdprior = dgamma(sd, 0.5, 0.5, log = T)
    return(aprior+bprior+cprior+sdprior)
}
```


Gibb's Sampler

```{r}
#Modify Iterations
# Initialise parameters and variables
n_iter <- 100000  # Number of iterations
burn_in <- 10000  # Number of burn-in iterations
```

```{r}
#Modify Iterations
# Initialise parameters and variables
n_iter <- 5000  # Number of iterations
burn_in <- 1000  # Number of burn-in iterations
```


```{r}
#Investigate Priors
# Initialise parameters and variables
tic('GS') #Start timing
set.seed(21)  #Reproducibility


alpha_samples <- beta_samples <- beta2_samples <- tau_samples <- numeric(n_iter)

# Gibbs sampling
for (i in 1:n_iter) {
  # Sample alpha
  alpha_var <- 1 / ((1 / alphavar) + n * tau)
  alpha_mean <- alpha_var * (alphamean/alphavar + tau * sum(y - beta*x - beta2*x2))
  alpha <- rnorm(1, mean = alpha_mean, sd = sqrt(alpha_var))
  
  # Sample beta1
  beta_var <- 1 / (1/betavar + tau * sum(x^2))
  beta_mean <- beta_var * (betamean/betavar+ tau * sum(x * (y - alpha - beta2 * x2)))
  beta <- rnorm(1, mean = beta_mean, sd = sqrt(beta_var))
  
  # Sample beta2
  beta2_var <- 1 / (1/beta2var + tau * sum(x2^2))
  beta2_mean <- beta2_var * (beta2mean/beta2var+tau * sum(x2 * (y - alpha - beta * x)))
  beta2 <- rnorm(1, mean = beta2_mean, sd = sqrt(beta2_var))
  
  # Sample tau
  tau <- runif(1, min = 0, max = 10)
  
  # Store samples
  alpha_samples[i] <- alpha
  beta_samples[i] <- beta
  beta2_samples[i] <- beta2
  tau_samples[i] <- tau
}

# Discard burn-in samples
alpha_samples <- alpha_samples[-(1:burn_in)]
beta_samples <- beta_samples[-(1:burn_in)]
beta2_samples <- beta2_samples[-(1:burn_in)]
tau_samples <- tau_samples[-(1:burn_in)]

# Calculate credible intervals for each parameter
credible_intervals_alpha <- quantile(alpha_samples, c(0.025, 0.975))
credible_intervals_beta <- quantile(beta_samples, c(0.025, 0.975))
credible_intervals_beta2 <- quantile(beta2_samples, c(0.025, 0.975))
credible_intervals_tau <- quantile(tau_samples, c(0.025, 0.975))

# Print credible intervals
print("Credible Intervals:")
print(paste("Alpha:", credible_intervals_alpha[1], "-", credible_intervals_alpha[2]))
print(paste("Beta1:", credible_intervals_beta[1], "-", credible_intervals_beta[2]))
print(paste("Beta2:", credible_intervals_beta2[1], "-", credible_intervals_beta2[2]))
print(paste("Tau:", credible_intervals_tau[1], "-", credible_intervals_tau[2]))

toc()

# Plot histograms
par(mfrow = c(2, 2))
hist(alpha_samples, main = "Alpha")
hist(beta_samples, main = "Beta1")
hist(beta2_samples, main = "Beta2")
hist(tau_samples, main = "Tau")
par(mfrow = c(1, 1))


summary(alpha_samples)
summary(beta_samples)
summary(beta2_samples)
summary(tau_samples)


gsposterior_means = cbind(mean(alpha_samples), mean(beta_samples), mean(beta2_samples), mean(tau_samples))
gsposterior_means
quantile(alpha_samples,0.025)
quantile(alpha_samples,0.975)

quantile(beta_samples,0.025)
quantile(beta_samples,0.975)

quantile(beta2_samples,0.025)
quantile(beta2_samples,0.975)

quantile(tau_samples,0.025)
quantile(tau_samples,0.975)

gsposterior1<- cbind((alpha_samples), (beta_samples), (beta2_samples), (tau_samples))


par(mfrow = c(2,2))
hist(gsposterior1[,1],nclass=20, main="Posterior of Alpha", xlab="True value = red line" )
abline(v = mean(gsposterior1[,1]), col="blue")
abline(v = truealpha, col="red" )
hist(gsposterior1[,2],nclass=20, main="Posterior of Beta1", xlab="True value = red line" )
abline(v = mean(gsposterior1[,2]), col="blue")
abline(v = truebeta, col="red" )
hist(gsposterior1[,3],nclass=20, main="Posterior of Beta2", xlab="True value = red line" )
abline(v = mean(gsposterior1[,3]), col="blue")
abline(v = truebeta2, col="red" )
hist(gsposterior1[,4],nclass=20, main="Posterior of Tau", xlab="True value = red line" )
abline(v = mean(gsposterior1[,4]), col="blue")
abline(v = truetau, col="red" )

```


For all methods, modify dataset

```{r}
#Modify Dataset
```




Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
